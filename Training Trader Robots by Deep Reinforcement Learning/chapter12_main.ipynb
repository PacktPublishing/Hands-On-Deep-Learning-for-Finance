{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import talib\n",
    "\n",
    "from Agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load NASDAQ-100 data\n",
    "data_folder = '../Index Replication by Auto-encoders/data/'\n",
    "alldata = pd.read_pickle(data_folder + 'nasdaq100_6y.pkl')\n",
    "\n",
    "# Stock symbol\n",
    "symbol = 'AAL'\n",
    "# Load close prices of to NumPy array\n",
    "close = alldata[symbol].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(close):\n",
    "    \"\"\"\n",
    "    Function prepare_dataset to generate input data and trading strategy from stock close prices\n",
    "    \"\"\"\n",
    "\n",
    "    macd, macdsignal, macdhist = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "    # Cancel NaN values\n",
    "    macdhist = macdhist[~np.isnan(macdhist)]\n",
    "    macd = macd[-len(macdhist):]\n",
    "    macdsignal = macdsignal[-len(macdhist):]\n",
    "\n",
    "    # Scaling features to a range [0, 1]\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    macdhist_norm = min_max_scaler.fit_transform(np.expand_dims(macdhist, axis=1))\n",
    "\n",
    "    # Implement strategy\n",
    "    start_sell = 0.4\n",
    "    stop_sell = 0.1\n",
    "    start_buy = -0.4\n",
    "    stop_buy = -0.1\n",
    "\n",
    "    y = np.full(len(macdhist), np.nan)\n",
    "    y[0] = 0\n",
    "\n",
    "    for i in range(1, len(macdhist)):\n",
    "\n",
    "        if y[i-1] == 0:\n",
    "            if (macdhist_norm[i] >= start_sell):\n",
    "                # Enter sell position\n",
    "                y[i] = -1\n",
    "            elif (macdhist_norm[i] <= start_buy):\n",
    "                # Enter buy position\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "        elif y[i-1] == -1:\n",
    "            if macdhist_norm[i] > stop_sell:\n",
    "                # Stay in sell position\n",
    "                y[i] = -1\n",
    "            else:\n",
    "                # Leave sell position\n",
    "                y[i] = 0\n",
    "        else:\n",
    "            if macdhist_norm[i] < stop_buy:\n",
    "                # Stay in buy position\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                # Leave buy position\n",
    "                y[i] = 0\n",
    "\n",
    "    # Plot strategy\n",
    "    dates = np.arange(len(macdhist))\n",
    "    plt.plot(dates, y,'g', label='Strategy Positions')\n",
    "    plt.bar(dates, macdhist_norm[:, 0], width=1, color='blue', label='MACD histogram')\n",
    "    plt.plot(dates, start_sell * np.ones(len(macdhist)), 'k--', lw=1)\n",
    "    plt.plot(dates, stop_sell * np.ones(len(macdhist)), 'k--', lw=1)\n",
    "    plt.plot(dates, start_buy * np.ones(len(macdhist)), 'k--', lw=1)\n",
    "    plt.plot(dates, stop_buy * np.ones(len(macdhist)), 'k--', lw=1)\n",
    "    plt.xlabel('Days')\n",
    "    plt.xlim((300, 600))\n",
    "    plt.legend()\n",
    "    plt.savefig('images/AAL_macd.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate input data - technical indicators\n",
    "    ind1 = talib.MIDPOINT(close)    # Overlap: MidPoint over period\n",
    "    ind2 = talib.HT_DCPERIOD(close) # Cycle Indicator Functions:  Hilbert Transform - Dominant Cycle Period\n",
    "    ind3 = talib.MAX(close)         # Math Operator: Highest value over a specified period\n",
    "    ind4 = talib.SIN(close)         # Math Transform: Vector Trigonometric Sin\n",
    "    ind5 = talib.APO(close)         # Momentum: Absolute Price Oscillator\n",
    "\n",
    "    x = np.vstack((macdhist, macd, macdsignal, ind1[-len(macdhist):], ind2[-len(macdhist):],\n",
    "                   ind3[-len(macdhist):], ind4[-len(macdhist):], ind5[-len(macdhist):]))\n",
    "    x = x.T\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement MACD trading strategy\n",
    "X, y = prepare_dataset(close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "n_train = int(X.shape[0] * 0.8)\n",
    "\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode trading signal with integers between 0 and n-1 classes\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create and train the Agent\n",
    "# Variable definiton\n",
    "episodes = 25\n",
    "look_back = 15\n",
    "batch_size = 32\n",
    "action_size = len(le.classes_)\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Agent\n",
    "agent = Agent(look_back, action_size, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(agent, dataX, dataY, episodes, look_back):\n",
    "    \"\"\"\n",
    "    Function run to train the agent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Length of dataset\n",
    "    times = len(dataX)\n",
    "\n",
    "    # List of total rewards\n",
    "    total_reward_list = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "\n",
    "        # print('Episode: ' + str(ep))\n",
    "        \n",
    "        # Initial state and position\n",
    "        state = dataX[:look_back, :][np.newaxis, :, :]\n",
    "        pos = dataY[look_back - 1]\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(1, times - look_back + 1):\n",
    "\n",
    "            # Predict action based on the current state\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Calculate reward\n",
    "            if action == pos:   # 0:-1      1:0     2:1\n",
    "                reward = +1\n",
    "\n",
    "            elif (pos == 0 or pos == 2):\n",
    "                if action == 1:\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                reward = -1\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Final state\n",
    "            if t == times - look_back:\n",
    "                done = True\n",
    "\n",
    "            # Receive next state and position\n",
    "            next_state = dataX[t:t + look_back, :][np.newaxis, :, :]\n",
    "            next_pos = dataY[t + look_back - 1]\n",
    "            \n",
    "            # Remember current experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Make next_state the new current state; the same for pos\n",
    "            state = next_state\n",
    "            pos = next_pos\n",
    "\n",
    "            if done:\n",
    "                print('Episode: %i ---> Total Reward: %i' %(ep, total_reward))\n",
    "                total_reward_list.append(total_reward)\n",
    "\n",
    "            # Train the agent with previous experiences\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "        if (ep + 1) % 5 == 0 and ep > 0:\n",
    "            file = 'AAL_robot_checkpoint' + str(ep + 1)\n",
    "            # Serialize weights to HDF5\n",
    "            agent.model.save_weights(file + \".h5\")\n",
    "            # Save epsilon\n",
    "            pickle.dump(agent.epsilon, open(file + \"_epsilon.pickle\", \"wb\"))\n",
    "\n",
    "    # Save list of rewards along the epochs\n",
    "    np.savetxt(file + '_total_reward.txt', total_reward_list)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Agent\n",
    "run(agent, X_train, y_train, episodes, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load rewards\n",
    "total_reward_list = np.loadtxt('checkpoints/AAL_robot_checkpoint' + str(episodes) + '_total_reward.txt')\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, episodes+1), total_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, dataX, dataY, look_back):\n",
    "    \"\"\"\n",
    "    Function run to evaluate the trained agent\n",
    "    \"\"\"\n",
    "    \n",
    "    # Length of dataset\n",
    "    times = len(dataX)\n",
    "\n",
    "    # Initial state and position\n",
    "    state = dataX[:look_back, :][np.newaxis, :, :]\n",
    "    pos = dataY[look_back - 1]\n",
    "\n",
    "    # List of predicted positions\n",
    "    pos_list = []\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    for t in range(1, times - look_back + 1):\n",
    "\n",
    "        # Predict action based on the current state\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Calculate reward\n",
    "        if action == pos:   # 0:-1      1:0     2:1\n",
    "            reward = +1\n",
    "\n",
    "        elif (pos == 0 or pos == 2):\n",
    "            if action == 1:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = -1\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        pos_list.append(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Final state\n",
    "        if t == times - look_back:\n",
    "            done = True\n",
    "\n",
    "        # Receive next state and position\n",
    "        next_state = dataX[t:t + look_back, :][np.newaxis, :, :]\n",
    "        next_pos = dataY[t + look_back - 1]\n",
    "\n",
    "        # Remember current experience\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Make next_state the new current state; the same for pos\n",
    "        state = next_state\n",
    "        pos = next_pos\n",
    "\n",
    "        if done:\n",
    "            print('Total Reward: %i' % total_reward)\n",
    "\n",
    "    return np.array(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Make predictions\n",
    "y_pred_test = evaluate(agent, X_test, y_test, look_back)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "acc = accuracy_score(y_test[look_back-1:-1], y_pred_test)\n",
    "\n",
    "print('Accuracy: %.2f %%' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate and print precision, recall, f1 score and support\n",
    "p, r, f1, s = precision_recall_fscore_support(y_test[look_back-1:-1], y_pred_test, average=None)\n",
    "results = pd.DataFrame({'1-Precision': p, '2-Recall': r, '3-F1 score': f1, '4-Support': s}, index=le.classes_)\n",
    "\n",
    "print(results.round(decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decodificate labels\n",
    "y_true_test = le.inverse_transform(y_test[look_back-1:-1])\n",
    "y_pred_test = le.inverse_transform(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot strategy\n",
    "plt.figure()\n",
    "plt.plot(y_true_test, label='true')\n",
    "plt.plot(y_pred_test, label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
